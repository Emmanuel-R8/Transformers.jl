<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Basic · Transformers.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../"><img class="logo" src="../assets/logo.svg" alt="Transformers.jl logo"/></a><h1>Transformers.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../tutorial/">Tutorial</a></li><li class="current"><a class="toctext" href>Basic</a><ul class="internal"><li><a class="toctext" href="#Transformer-1">Transformer</a></li><li><a class="toctext" href="#Positionwise-1">Positionwise</a></li><li><a class="toctext" href="#PositionEmbedding-1">PositionEmbedding</a></li><li><a class="toctext" href="#API-Reference-1">API Reference</a></li></ul></li><li><a class="toctext" href="../stacks/">Stacks</a></li><li><a class="toctext" href="../pretrain/">Pretrain</a></li><li><span class="toctext">Models</span><ul><li><a class="toctext" href="../gpt/">GPT</a></li><li><a class="toctext" href="../bert/">BERT</a></li></ul></li><li><a class="toctext" href="../datasets/">Datasets</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Basic</a></li></ul><a class="edit-page" href="https://github.com/chengchingwen/Transformers.jl/blob/master/docs/src/basic.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Basic</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Transformers.Basic-1" href="#Transformers.Basic-1">Transformers.Basic</a></h1><p>Basic functionality of Transformers.jl, provide the Transformer encoder/decoder implementation and other convenient function.</p><h2><a class="nav-anchor" id="Transformer-1" href="#Transformer-1">Transformer</a></h2><p>The <code>Transformer</code> and <code>TransformerDecoder</code> is the encoder and decoder block of the origin paper, and they are all implement as the  regular Flux Layer, so you can treat them just as the <code>Dense</code> layer. See the docstring for the argument. However, for the sequence  data input, we usually have a 3 dimensional input of shape <code>(hidden size, sequence length, batch size)</code> instead of just <code>(hidden size, batch size)</code>.  Therefore, we implement both 2d &amp; 3d operation according to the input type (The <code>N</code> of <code>Array{T, N}</code>). We are able to handle both input of shape  <code>(hidden size, sequence length, batch size)</code> and <code>(hidden size, sequence length)</code> for the case with only 1 input.</p><pre><code class="language-julia">using Transfomers

m = Transformer(512, 8, 64, 2048) #define a Transformer block with 8 head and 64 neuron for each head
x = randn(512, 30, 3) #fake data of length 30

y = m(x)</code></pre><h2><a class="nav-anchor" id="Positionwise-1" href="#Positionwise-1">Positionwise</a></h2><p>For the sequential task, we need to handle the 3 dimensional input. However, most of the layer in Flux only support input with shape  <code>(hidden size, batch size)</code>. In order to tackle this problem, we implement the <code>Positionwise</code> helper function that is almost the same  as <code>Flux.Chain</code> but it will run the model position-wisely. (internally it just reshape the input to 2d and apply the model then reshape  back). </p><pre><code class="language-julia">using Transformers
using Flux

m = Positionwise(Dense(10, 5), Dense(5, 2), softmax)
x = randn(10, 30, 3)

y = m(x)

# which is equivalent to 
# 
# m = Chain(Dense(10, 5), Dense(5, 2), softmax)
# x1 = randn(10, 30)
# x2 = randn(10, 30)
# x3 = randn(10, 30)
# y = cat(m(x1), m(x2), m(x3); dims=3)</code></pre><h2><a class="nav-anchor" id="PositionEmbedding-1" href="#PositionEmbedding-1">PositionEmbedding</a></h2><p>We implement two kinds of position embedding, one is based on the sin/cos function (mentioned in the paper,  attention is all you need). Another one is just like regular word embedding but with the position index. The  first argument is the <code>size</code>. Since the position embedding is only related to the length of the input ( we use <code>size(input, 2)</code> as the length), the return value of the layer will be the embedding of the given  length without duplicate to the batch size. you can/should use broadcast add to get the desired output.</p><pre><code class="language-julia"># sin/cos based position embedding which is not trainable
pe = PositionEmbedding(10) # or PositionEmbedding(10; trainable = false)

# trainable position embedding need to specify the maximum length
pe = PositionEmbedding(10, 1024; trainable = true)

x = randn(Float32, 10, 6, 3) #fake data of shape (10, length = 6, batched_size = 3)

e = pe(x) #get the position embedding
y = x .+ e # add the position embedding to each sample</code></pre><h2><a class="nav-anchor" id="API-Reference-1" href="#API-Reference-1">API Reference</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.CompositeEmbedding-Tuple{}" href="#Transformers.Basic.CompositeEmbedding-Tuple{}"><code>Transformers.Basic.CompositeEmbedding</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">CompositeEmbedding(;postprocessor=identity, es...)</code></pre><p>composite several embedding into one embedding according the aggregate methods and apply <code>postprocessor</code> on it.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/embeds/etype.jl#L14-L18">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.Embed" href="#Transformers.Basic.Embed"><code>Transformers.Basic.Embed</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Embed(size::Int, vocab_size::Int)</code></pre><p>The Embedding Layer, <code>size</code> is the hidden size. <code>vocab_size</code> is the number of the vocabulary. Just a wrapper for embedding matrix.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/embeds/embed.jl#L1-L5">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.PositionEmbedding" href="#Transformers.Basic.PositionEmbedding"><code>Transformers.Basic.PositionEmbedding</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">PositionEmbedding(size::Int, max_len::Int = 1024; trainable::Bool = false)</code></pre><p>The position embedding layer. <code>size</code> is the number of neuron. <code>max_len</code> is the maximum acceptable length of input. If is not <code>trainable</code>, <code>max_len</code> will dynamically adjust to the longest input length. If <code>trainable</code>, use a random init embedding value, otherwise use a sin/cos position encoding.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/embeds/position_embed.jl#L1-L7">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.Positionwise" href="#Transformers.Basic.Positionwise"><code>Transformers.Basic.Positionwise</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Positionwise(layers)</code></pre><p>just like <code>Flux.Chain</code>, but reshape input to 2d and reshape back when output. Work exactly the same as <code>Flux.Chain</code> when input is 2d array.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/extend3d.jl#L26-L31">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.Transformer-Tuple{Int64,Int64,Int64}" href="#Transformers.Basic.Transformer-Tuple{Int64,Int64,Int64}"><code>Transformers.Basic.Transformer</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">Transformer(size::Int, head::Int, ps::Int;
            future::Bool = true, act = relu, pdrop = 0.1)
Transformer(size::Int, head::Int, hs::Int, ps::Int;
            future::Bool = true, act = relu, pdrop = 0.1)</code></pre><p>Transformer layer.</p><p><code>size</code> is the input size. if <code>hs</code> is not specify, use <code>div(size, head)</code> as the hidden size of multi-head attention.  <code>ps</code> is the hidden size &amp; <code>act</code> is the activation function of the positionwise feedforward layer.  When <code>future</code> is <code>false</code>, the k-th token can&#39;t see the j-th tokens where j &gt; k. <code>pdrop</code> is the dropout rate.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/transformer.jl#L36-L47">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.TransformerDecoder-Tuple{Int64,Int64,Int64}" href="#Transformers.Basic.TransformerDecoder-Tuple{Int64,Int64,Int64}"><code>Transformers.Basic.TransformerDecoder</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">TransformerDecoder(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)
TransformerDecoder(size::Int, head::Int, hs::Int, ps::Int; act = relu, pdrop = 0.1)</code></pre><p>TransformerDecoder layer. Decode the value from a Encoder.</p><p><code>size</code> is the input size. if <code>hs</code> is not specify, use <code>div(size, head)</code> as the hidden size of multi-head attention.  <code>ps</code> is the hidden size &amp; <code>act</code> is the activation function of the positionwise feedforward layer.  <code>pdrop</code> is the dropout rate.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/transformer.jl#L108-L117">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.TransformerModel" href="#Transformers.Basic.TransformerModel"><code>Transformers.Basic.TransformerModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">TransformerModel(embed::AbstractEmbed, transformers::AbstractTransformer)
TransformerModel(
                  embed::AbstractEmbed,
                  transformers::AbstractTransformer,
                  classifier
                 )</code></pre><p>a structure for putting everything together</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/model.jl#L3-L12">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.Vocabulary" href="#Transformers.Basic.Vocabulary"><code>Transformers.Basic.Vocabulary</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Vocabulary{T}(voc::Vector{T}, unk::T) where T</code></pre><p>struct for holding the vocabulary list to encode/decode input tokens.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/embeds/vocab.jl#L1-L5">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.Vocabulary-Tuple{Any}" href="#Transformers.Basic.Vocabulary-Tuple{Any}"><code>Transformers.Basic.Vocabulary</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">(vocab::Vocabulary)(x)</code></pre><p>encode the given data to the index encoding.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/embeds/vocab.jl#L47-L51">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.clear_classifier-Tuple{TransformerModel}" href="#Transformers.Basic.clear_classifier-Tuple{TransformerModel}"><code>Transformers.Basic.clear_classifier</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">clear_classifier(model::TransformerModel)</code></pre><p>return a new TransformerModel without classifier.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/model.jl#L30-L34">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.encode-Union{Tuple{W}, Tuple{T}, Tuple{Vocabulary{T},Union{T, W}}} where W where T" href="#Transformers.Basic.encode-Union{Tuple{W}, Tuple{T}, Tuple{Vocabulary{T},Union{T, W}}} where W where T"><code>Transformers.Basic.encode</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">encode(vocab::Vocabulary, x)</code></pre><p>encode the given data to the index encoding.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/embeds/vocab.jl#L27-L31">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.gather-Union{Tuple{T}, Tuple{AbstractArray{T,2},AbstractArray{Int64,N} where N}} where T" href="#Transformers.Basic.gather-Union{Tuple{T}, Tuple{AbstractArray{T,2},AbstractArray{Int64,N} where N}} where T"><code>Transformers.Basic.gather</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">gather(w::AbstractMatrix{T}, xs) where</code></pre><p>getting vector at the given indices, <code>xs</code> is a array of indices(<code>Int</code> type).</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/embeds/gather.jl#L8-L13">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.gather-Union{Tuple{T}, Tuple{AbstractArray{T,2},OneHotArray}} where T" href="#Transformers.Basic.gather-Union{Tuple{T}, Tuple{AbstractArray{T,2},OneHotArray}} where T"><code>Transformers.Basic.gather</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">gather(w::AbstractMatrix{T}, xs::OneHotArray) where</code></pre><p>getting vector at the given onehot encoding.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/embeds/gather.jl#L1-L6">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.gather-Union{Tuple{T}, Tuple{AbstractArray{T,N} where N,AbstractArray{#s17,N} where N where #s17&lt;:Tuple}} where T" href="#Transformers.Basic.gather-Union{Tuple{T}, Tuple{AbstractArray{T,N} where N,AbstractArray{#s17,N} where N where #s17&lt;:Tuple}} where T"><code>Transformers.Basic.gather</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">gather(w::AbstractArray{T}, xs) where</code></pre><p>getting vector at the given indices, <code>xs</code> is a array of cartesian indices(<code>Tuple{Int}</code> type).</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/embeds/gather.jl#L23-L28">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.getmask-Tuple{Union{Array{#s17,1}, Tuple{Vararg{#s17,N}}} where N where #s17&lt;:(Union{Array{T,1}, Tuple{Vararg{T,N}}} where N where T)}" href="#Transformers.Basic.getmask-Tuple{Union{Array{#s17,1}, Tuple{Vararg{#s17,N}}} where N where #s17&lt;:(Union{Array{T,1}, Tuple{Vararg{T,N}}} where N where T)}"><code>Transformers.Basic.getmask</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">getmask(ls::Container{&lt;:Container})</code></pre><p>get the mask for batched data.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/embeds/vocab.jl#L99-L103">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.getmask-Union{Tuple{A}, Tuple{A,A}} where A&lt;:(AbstractArray{T,3} where T)" href="#Transformers.Basic.getmask-Union{Tuple{A}, Tuple{A,A}} where A&lt;:(AbstractArray{T,3} where T)"><code>Transformers.Basic.getmask</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">getmask(m1::A, m2::A) where A &lt;: Abstract3DTensor</code></pre><p>get the mask for the covariance matrix.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/embeds/vocab.jl#L114-L118">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.logcrossentropy-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3},Any}} where T" href="#Transformers.Basic.logcrossentropy-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3},Any}} where T"><code>Transformers.Basic.logcrossentropy</code></a> — <span class="docstring-category">Method</span>.</div><div><div><p>compute the cross entropy with mask where p is already the log(p)</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/loss.jl#L12">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.logcrossentropy-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3}}} where T" href="#Transformers.Basic.logcrossentropy-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3}}} where T"><code>Transformers.Basic.logcrossentropy</code></a> — <span class="docstring-category">Method</span>.</div><div><div><p>compute the cross entropy where p is already the log(p)</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/loss.jl#L33">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.logkldivergence-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3},Any}} where T" href="#Transformers.Basic.logkldivergence-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3},Any}} where T"><code>Transformers.Basic.logkldivergence</code></a> — <span class="docstring-category">Method</span>.</div><div><div><p>compute the kl divergence with mask where p is already the log(p)</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/loss.jl#L1">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.logkldivergence-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3}}} where T" href="#Transformers.Basic.logkldivergence-Union{Tuple{T}, Tuple{AbstractArray{T,3},AbstractArray{T,3}}} where T"><code>Transformers.Basic.logkldivergence</code></a> — <span class="docstring-category">Method</span>.</div><div><div><p>compute the kl divergence where p is already the log(p)</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/loss.jl#L23">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.onehot2indices-Tuple{OneHotArray}" href="#Transformers.Basic.onehot2indices-Tuple{OneHotArray}"><code>Transformers.Basic.onehot2indices</code></a> — <span class="docstring-category">Method</span>.</div><div><div><p>turn one hot encoding to indices</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/embeds/onehot.jl#L46">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.set_classifier-Tuple{TransformerModel,Any}" href="#Transformers.Basic.set_classifier-Tuple{TransformerModel,Any}"><code>Transformers.Basic.set_classifier</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">set_classifier(model::TransformerModel, classifier)</code></pre><p>return a new TransformerModel whose classifier is set to <code>classifier</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/model.jl#L23-L27">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.@toNd" href="#Transformers.Basic.@toNd"><code>Transformers.Basic.@toNd</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-julia">@toNd f(x, y, z...; a=a, b=b, c=c...) n</code></pre><p>macro for calling 2-d array function on N-d array by reshape input with reshape(x, size(x, 1), :) and reshape back with reshape(out, :, input[n][2:end]...) where n is the n-th input(default=1).</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/extend3d.jl#L6-L12">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.MultiheadAttention-NTuple{4,Int64}" href="#Transformers.Basic.MultiheadAttention-NTuple{4,Int64}"><code>Transformers.Basic.MultiheadAttention</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">MultiheadAttention(head::Int, is::Int, hs::Int, os::Int;
                   future::Bool=true, pdrop = 0.1)</code></pre><p>Multihead dot product Attention Layer, <code>head</code> is the number of head,  <code>is</code> is the input size, <code>hs</code> is the hidden size of input projection layer of each head,  <code>os</code> is the output size. When <code>future</code> is <code>false</code>, the k-th token can&#39;t see tokens at &gt; k.  <code>pdrop</code> is the dropout rate.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/mh_atten.jl#L33-L41">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Basic.PwFFN" href="#Transformers.Basic.PwFFN"><code>Transformers.Basic.PwFFN</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>just a wrapper for two dense layer.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/basic/transformer.jl#L14">source</a></section><footer><hr/><a class="previous" href="../tutorial/"><span class="direction">Previous</span><span class="title">Tutorial</span></a><a class="next" href="../stacks/"><span class="direction">Next</span><span class="title">Stacks</span></a></footer></article></body></html>
