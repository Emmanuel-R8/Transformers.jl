#=================== GPT
[gpt]
description = "Generative Pre-Training of Transformers"

[gpt.gpt]
host = "OpenAI"
description = "Origin GPT from finetune-transformer-lm"
link = "https://github.com/openai/finetune-transformer-lm"

[gpt.gpt.models.OpenAIftlm]
case = "uncased"
vocab = 40000
layer = 12
hidden = 768
head = 12
subword = "bpe"
name = "OpenAIftlm"
items = ["gpt_model", "bpe", "vocab", "tokenizer"]
url = "https://docs.google.com/uc?export=download&id=1jZ4wSrR7rGXprKz1a3O8PpEQ2o-nnMEA"
checksum = "e372ab1f42fff84efc6e97d30d0aadd823706e9b6d4084a8c474ba68c494e22c"

#=================== BERT
[bert]
description = "Bidirectional Encoder Representations from Transformers"

[bert.bert]
host = "Google"
description = "Origin BERT model."
link = "https://github.com/google-research/bert"
cite = """@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}"""

[bert.bert.models.uncased_L-12_H-768_A-12]
case = "uncased"
layer = 12
hidden = 768
head = 12
subword = "wordpiece"
name = "uncased_L-12_H-768_A-12"
items = ["bert_model", "wordpiece", "tokenizer"]
url = "https://docs.google.com/uc?export=download&id=1X-p84u7LDEJlhAnDXnyrXbbFo9Y0UBzu"
checksum = "631de5b205c8cb535818ab3a0a2c25c8ac66d841197225496bc26d0ae9fa5702"

[bert.bert.models.uncased_L-24_H-1024_A-16]
case = "uncased"
layer = 24
hidden = 1024
head = 16
subword = "wordpiece"
name = "uncased_L-24_H-1024_A-16"
items = ["bert_model", "wordpiece", "tokenizer"]
url = "https://docs.google.com/uc?export=download&id=1IR7RWI2ZORzR7JmPSPrUtZk9jdcOm0Na"
checksum = "c696f7fa9c9a774e61d5bff83bd2a0f2263698591be3344400a58c3c88af8e8f"

[bert.bert.models.wwm_cased_L-24_H-1024_A-16]
case = "cased"
layer = 24
hidden = 1024
head = 16
subword = "wordpiece"
name = "wwm_cased_L-24_H-1024_A-16"
items = ["bert_model", "wordpiece", "tokenizer"]
desc = "trained with whole word masked."
url = "https://docs.google.com/uc?export=download&id=1sqVrc_PdhYQzdt903yvx7OROtmpjRLUi"
checksum = "1d77c9128d4262f58aea8d2aa6d21becba241f607168c24e2fc76163d0db7b0a"

[bert.bert.models.wwm_uncased_L-24_H-1024_A-16]
case = "cased"
layer = 24
hidden = 1024
head = 16
subword = "wordpiece"
name = "wwm_uncased_L-24_H-1024_A-16"
items = ["bert_model", "wordpiece", "tokenizer"]
desc = "trained with whole word masked."
url = "https://docs.google.com/uc?export=download&id=1FrpWNCIgVCNGTF-rlU8_3oj1Xp4S94e9"
checksum = "391b8721471dae54780ea61f1edd286ce2fa67aa9ff47a28807e96b006ef21df"

[bert.bert.models.multilingual_L-12_H-768_A-12]
case = "multilingual"
layer = 12
hidden = 768
head = 12
subword = "wordpiece"
name = "multilingual_L-12_H-768_A-12"
items = ["bert_model", "wordpiece", "tokenizer"]
url = "https://docs.google.com/uc?export=download&id=1BJUHaVkpnDL71VlVJS2yRjNF72l1LmWD"
checksum = "cb2a1b24b4b99434932c647c21baab4ae7867b45053b2a804d8213e9bdc2e04e"

[bert.bert.models.multi_cased_L-12_H-768_A-12]
case = "multi_cased"
layer = 12
hidden = 768
head = 12
subword = "wordpiece"
name = "multi_cased_L-12_H-768_A-12"
items = ["bert_model", "wordpiece", "tokenizer"]
url = "https://docs.google.com/uc?export=download&id=1teAmrvOqJfyEa2yYFyNLW7M8JOmg-4sn"
checksum = "ee239ec5fc30ce0c817626dbbf4b8f25119a1b202930f7f1b50ee88b98f26647"

[bert.bert.models.chinese_L-12_H-768_A-12]
case = "chinese"
layer = 12
hidden = 768
head = 12
subword = "wordpiece"
name = "chinese_L-12_H-768_A-12"
items = ["bert_model", "wordpiece", "tokenizer"]
url = "https://docs.google.com/uc?export=download&id=14OaijyaNjpK69-Oh7NddIXFlLrgfkf35"
checksum = "94cf78daa24cc3652deec24c786cbe8067fe8dec824d34d7280a0ca9069dc8a1"

[bert.bert.models.cased_L-24_H-1024_A-16]
case = "cased"
layer = 24
hidden = 1024
head = 16
subword = "wordpiece"
name = "cased_L-24_H-1024_A-16"
items = ["bert_model", "wordpiece", "tokenizer"]
url = "https://docs.google.com/uc?export=download&id=1TsaVQHlhOVG905elbHXeXK2qzp-vFe68"
checksum = "d08bd9011df02afcf324fc8888304e4868f5bbfca061b4396262903c7d7a7fe0"

[bert.bert.models.cased_L-12_H-768_A-12]
case = "cased"
layer = 12
hidden = 768
head = 12
subword = "wordpiece"
name = "cased_L-12_H-768_A-12"
items = ["bert_model", "wordpiece", "tokenizer"]
url = "https://docs.google.com/uc?export=download&id=1p4mej6xhfpqzLfRuMNkKHJb9ScjXU193"
checksum = "a02aadcc30c4e49355ebe350b3933dcdcecf7e50af3ea442b71e3c728423ac3a"

#=================== SciBERT
[bert.scibert]
host = "Allenai"
description = "A BERT model for scientific text."
link = "https://github.com/allenai/scibert"
cite = """@inproceedings{Beltagy2019SciBERT,
  title={SciBERT: Pretrained Language Model for Scientific Text},
  author={Iz Beltagy and Kyle Lo and Arman Cohan},
  year={2019},
  booktitle={EMNLP},
  Eprint={arXiv:1903.10676}
}"""


[bert.scibert.models.scibert_scivocab_uncased]
case = "uncased"
layer = 12
hidden = 768
head = 12
subword = "wordpiece"
name = "scibert_scivocab_uncased"
items = ["bert_model", "wordpiece", "tokenizer"]
desc = "vocabularies from scientific corpus."
url = "https://docs.google.com/uc?export=download&id=1L0woI2DeNES5OCzgLNBOCRrj5ikOzN7c"
checksum = "aa87890fe7c56a30ec4df79cdfabc575448efb6f31e119e6dc0d53150940b1ea"

[bert.scibert.models.scibert_scivocab_cased]
case = "cased"
layer = 12
hidden = 768
head = 12
subword = "wordpiece"
name = "scibert_scivocab_cased"
items = ["bert_model", "wordpiece", "tokenizer"]
desc = "vocabularies from scientific corpus."
url = "https://docs.google.com/uc?export=download&id=1YYy6cH_gQf9rXmnPW9861N0Chlf_ZmTF"
checksum = "2d6b0e839d5485dc9e5eedcc967671f134308316fe3ee3e9e249cc2aa502f1c4"

[bert.scibert.models.scibert_basevocab_uncased]
case = "uncased"
layer = 12
hidden = 768
head = 12
subword = "wordpiece"
name = "scibert_basevocab_uncased"
items = ["bert_model", "wordpiece", "tokenizer"]
url = "https://docs.google.com/uc?export=download&id=16EgAh0uo7pB7aQKCMeI5dfMkCJ1y-RvB"
checksum = "216afe3542536ac32579f8b81c2b3564c2a8a0033a214274ddacecf215b97100"

[bert.scibert.models.scibert_basevocab_cased]
case = "cased"
layer = 12
hidden = 768
head = 12
subword = "wordpiece"
name = "scibert_basevocab_cased"
items = ["bert_model", "wordpiece", "tokenizer"]
url = "https://docs.google.com/uc?export=download&id=1Htg-qTj03YRQqBgbHxz8KOULBYQGQieP"
checksum = "e5a3b2a692e3971185979c0d8c6882f092380bd580db79a32e72e1b4ec77537f"

#=================== 