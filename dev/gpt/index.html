<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>GPT · Transformers.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Transformers.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../basic/">Basic</a></li><li><a class="toctext" href="../stacks/">Stacks</a></li><li><a class="toctext" href="../pretrain/">Pretrain</a></li><li><span class="toctext">Models</span><ul><li class="current"><a class="toctext" href>GPT</a><ul class="internal"><li><a class="toctext" href="#API-reference-1">API reference</a></li></ul></li><li><a class="toctext" href="../bert/">BERT</a></li></ul></li><li><a class="toctext" href="../datasets/">Datasets</a></li></ul></nav><article id="docs"><header><nav><ul><li>Models</li><li><a href>GPT</a></li></ul><a class="edit-page" href="https://github.com/chengchingwen/Transformers.jl/blob/master/docs/src/gpt.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>GPT</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Transformers.GenerativePreTrain-1" href="#Transformers.GenerativePreTrain-1">Transformers.GenerativePreTrain</a></h1><p>Implementation of gpt-1 model</p><h2><a class="nav-anchor" id="API-reference-1" href="#API-reference-1">API reference</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.GenerativePreTrain.Gpt-NTuple{4,Int64}" href="#Transformers.GenerativePreTrain.Gpt-NTuple{4,Int64}"><code>Transformers.GenerativePreTrain.Gpt</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">Gpt(size::Int, head::Int, ps::Int, layer::Int;
    act = gelu, pdrop = 0.1, attn_pdrop = 0.1)
Gpt(size::Int, head::Int, hs::Int, ps::Int, layer::Int;
    act = gelu, pdrop = 0.1, attn_pdrop = 0.1)</code></pre><p>the Generative Pretrained Transformer(GPT) model.</p><pre><code class="language-none">(gpt::Gpt)(x::T, mask=nothing; all::Bool=false)</code></pre><p>eval the gpt layer on input <code>x</code>. If <code>mask</code> is given, mask the attention with <code>mask</code>. Moreover, set <code>all</code> to <code>true</code> to get all  outputs of each transformer layer.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/cfe559750fd616958b2c7852d6a6096829f379ae/src/gpt/gpt.jl#L17-L29">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.GenerativePreTrain.lmloss-Union{Tuple{T}, Tuple{Embed{T,W} where W&lt;:(AbstractArray{T,N} where N),OneHotArray,AbstractArray{T,N} where N,Any}} where T" href="#Transformers.GenerativePreTrain.lmloss-Union{Tuple{T}, Tuple{Embed{T,W} where W&lt;:(AbstractArray{T,N} where N),OneHotArray,AbstractArray{T,N} where N,Any}} where T"><code>Transformers.GenerativePreTrain.lmloss</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">lmloss(embed, onehot, encoding, mask)</code></pre><p>compute the language modeling loss for Gpt, onehot is the onehot array of the origin input sentence. encoding the output of Gpt, mask is the mask between input sentences.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/cfe559750fd616958b2c7852d6a6096829f379ae/src/gpt/gpt.jl#L62-L68">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.GenerativePreTrain.load_gpt_pretrain" href="#Transformers.GenerativePreTrain.load_gpt_pretrain"><code>Transformers.GenerativePreTrain.load_gpt_pretrain</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">load_gpt_pretrain(path::AbstractString, sym = :all; startsym=&quot;_start_&quot;, delisym=&quot;_delimiter_&quot;, clfsym=&quot;_classify_&quot;, unksym=&quot;&lt;unk&gt;&quot;)</code></pre><p>load gpt data/model from pretrain bson data. use <code>sym</code> to determine which data to load. set <code>&lt;xxx&gt;sym</code>s for setting special symbols in vocabulary. possible value: <code>:all</code>(default), <code>gpt_model</code>, <code>bpe</code>, <code>vocab</code>, <code>tokenizer</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/cfe559750fd616958b2c7852d6a6096829f379ae/src/gpt/load_pretrain.jl#L1-L6">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.GenerativePreTrain.gpt_tokenizer-Tuple{Any}" href="#Transformers.GenerativePreTrain.gpt_tokenizer-Tuple{Any}"><code>Transformers.GenerativePreTrain.gpt_tokenizer</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">gpt_tokenizer(x)</code></pre><p>An alternative for origin tokenizer (spacy tokenizer) used in gpt model.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/cfe559750fd616958b2c7852d6a6096829f379ae/src/gpt/tokenizer.jl#L23-L27">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.GenerativePreTrain.openAInpy2bson-Tuple{Any}" href="#Transformers.GenerativePreTrain.openAInpy2bson-Tuple{Any}"><code>Transformers.GenerativePreTrain.openAInpy2bson</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">openAInpy2bson(path; raw=false, saveto=&quot;./&quot;, startsym=&quot;_start_&quot;, delisym=&quot;_delimiter_&quot;, clfsym=&quot;_classify_&quot;, unksym=&quot;&lt;unk&gt;&quot;)</code></pre><p>turn openai released gpt format(npy) into BSON file. Set <code>raw</code> to <code>true</code> to remain the origin data format in bson.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/cfe559750fd616958b2c7852d6a6096829f379ae/src/gpt/npy2bson.jl#L10-L14">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.GenerativePreTrain.text_standardize-Tuple{Any}" href="#Transformers.GenerativePreTrain.text_standardize-Tuple{Any}"><code>Transformers.GenerativePreTrain.text_standardize</code></a> — <span class="docstring-category">Method</span>.</div><div><div><p>The function in the origin gpt code</p><p>fixes some issues the spacy tokenizer had on books corpus also does some whitespace standardization</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/cfe559750fd616958b2c7852d6a6096829f379ae/src/gpt/tokenizer.jl#L3-L8">source</a></section><footer><hr/><a class="previous" href="../pretrain/"><span class="direction">Previous</span><span class="title">Pretrain</span></a><a class="next" href="../bert/"><span class="direction">Next</span><span class="title">BERT</span></a></footer></article></body></html>
