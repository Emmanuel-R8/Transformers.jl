<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Pretrain · Transformers.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../"><img class="logo" src="../assets/logo.svg" alt="Transformers.jl logo"/></a><h1>Transformers.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../tutorial/">Tutorial</a></li><li><a class="toctext" href="../basic/">Basic</a></li><li><a class="toctext" href="../stacks/">Stacks</a></li><li class="current"><a class="toctext" href>Pretrain</a><ul class="internal"><li><a class="toctext" href="#using-Pretrains-1">using Pretrains</a></li><li><a class="toctext" href="#API-reference-1">API reference</a></li></ul></li><li><span class="toctext">Models</span><ul><li><a class="toctext" href="../gpt/">GPT</a></li><li><a class="toctext" href="../bert/">BERT</a></li></ul></li><li><a class="toctext" href="../datasets/">Datasets</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Pretrain</a></li></ul><a class="edit-page" href="https://github.com/chengchingwen/Transformers.jl/blob/master/docs/src/pretrain.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Pretrain</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Transformers.Pretrain-1" href="#Transformers.Pretrain-1">Transformers.Pretrain</a></h1><p>Functions for download and loading pretrain models.</p><h2><a class="nav-anchor" id="using-Pretrains-1" href="#using-Pretrains-1">using Pretrains</a></h2><p>For GPT and BERT, we provide a simple api to get the released pretrain weight and load them into our Julia version Transformer implementation. </p><pre><code class="language-julia">using Transformers
using Transformers.Pretrain
using Transformers.GenerativePreTrain
using Transformers.BidirectionalEncoder

# disable cli download check
ENV[&quot;DATADEPS_ALWAYS_ACCEPT&quot;] = true

#load everything in the pretrain model
bert_model, wordpiece, tokenizer = pretrain&quot;Bert-uncased_L-12_H-768_A-12&quot; 

#load model weight only
gpt_model = pretrain&quot;gpt-OpenAIftlm:gpt_model&quot;

#show the loaded model
show(bert_model)
show(gpt_model)</code></pre><pre><code class="language-none">TransformerModel{Bert}(
  embed = CompositeEmbedding(tok = Embed(768), segment = Embed(768), pe = PositionEmbedding(768, max_len=512), postprocessor = Positionwise(LayerNorm(768), Dropout{Float64}(0.1, true))),
  transformers = Bert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),
  classifier = 
    (
      pooler =&gt; Dense(768, 768, tanh)
      masklm =&gt; (
        transform =&gt; Chain(Dense(768, 768, gelu), LayerNorm(768))
        output_bias =&gt; TrackedArray{…,Array{Float32,1}}
      )
      nextsentence =&gt; Chain(Dense(768, 2), logsoftmax)
    )
)
TransformerModel{Gpt}(
  embed = CompositeEmbedding(tok = Embed(768), pe = PositionEmbedding(768, max_len=512)),
  transformers = Gpt(layers=12, head=12, head_size=64, pwffn_size=3072, size=768)
)</code></pre><p>The <code>pretrain&quot;&lt;model&gt;-&lt;model-name&gt;:&lt;item&gt;&quot;</code> string with <code>pretrain</code> prefix will load the specific item from a known pretrain file (see the list below).  The <code>&lt;model&gt;</code> is matched case insensitively, so not matter <code>bert</code>, <code>Bert</code>, <code>BERT</code>, or even <code>bErT</code> will find the BERT pretrain model. On the other hand,  the <code>&lt;model-name&gt;</code>, and <code>&lt;item&gt;</code> should be exactly the one on the list. See <code>example</code>.</p><p>Currently support pretrain:</p><table><tr><th style="text-align: right">model</th><th style="text-align: left">model name</th><th style="text-align: left">support items</th></tr><tr><td style="text-align: right">GPT</td><td style="text-align: left"><code>OpenAIftlm</code></td><td style="text-align: left">gpt_model, bpe, vocab, tokenizer</td></tr><tr><td style="text-align: right">Bert</td><td style="text-align: left"><code>uncased_L-24_H-1024_A-16</code></td><td style="text-align: left">bert_model, wordpiece, tokenizer</td></tr><tr><td style="text-align: right">Bert</td><td style="text-align: left"><code>wwm_cased_L-24_H-1024_A-16</code></td><td style="text-align: left">bert_model, wordpiece, tokenizer</td></tr><tr><td style="text-align: right">Bert</td><td style="text-align: left"><code>wwm_uncased_L-24_H-1024_A-16</code></td><td style="text-align: left">bert_model, wordpiece, tokenizer</td></tr><tr><td style="text-align: right">Bert</td><td style="text-align: left"><code>multilingual_L-12_H-768_A-12</code></td><td style="text-align: left">bert_model, wordpiece, tokenizer</td></tr><tr><td style="text-align: right">Bert</td><td style="text-align: left"><code>multi_cased_L-12_H-768_A-12</code></td><td style="text-align: left">bert_model, wordpiece, tokenizer</td></tr><tr><td style="text-align: right">Bert</td><td style="text-align: left"><code>chinese_L-12_H-768_A-12</code></td><td style="text-align: left">bert_model, wordpiece, tokenizer</td></tr><tr><td style="text-align: right">Bert</td><td style="text-align: left"><code>cased_L-24_H-1024_A-16</code></td><td style="text-align: left">bert_model, wordpiece, tokenizer</td></tr><tr><td style="text-align: right">Bert</td><td style="text-align: left"><code>cased_L-12_H-768_A-12</code></td><td style="text-align: left">bert_model, wordpiece, tokenizer</td></tr><tr><td style="text-align: right">Bert</td><td style="text-align: left"><code>uncased_L-12_H-768_A-12</code></td><td style="text-align: left">bert_model, wordpiece, tokenizer</td></tr></table><p>If you don&#39;t find a public pretrain you want on the list, please fire an issue.</p><p>See <code>example</code> folder for the complete example.</p><h2><a class="nav-anchor" id="API-reference-1" href="#API-reference-1">API reference</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Pretrain.load_pretrain-Tuple{Any}" href="#Transformers.Pretrain.load_pretrain-Tuple{Any}"><code>Transformers.Pretrain.load_pretrain</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">load_pretrain(name; kw...)</code></pre><p>same as <code>@pretrain_str</code>, but can pass keyword argument if needed.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/pretrain/Pretrain.jl#L47-L51">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Pretrain.pretrains" href="#Transformers.Pretrain.pretrains"><code>Transformers.Pretrain.pretrains</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>pretrains(query::String = &quot;&quot;; detailed::Bool = false)</p><p>Show all available models. you can also query a specific <code>model</code> or <code>model name</code>. show more detail with <code>detailed = true</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/pretrain/config.jl#L71-L76">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Transformers.Pretrain.@pretrain_str-Tuple{Any}" href="#Transformers.Pretrain.@pretrain_str-Tuple{Any}"><code>Transformers.Pretrain.@pretrain_str</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-julia">pretrain&quot;model-description:item&quot;</code></pre><p>convenient macro for loading data from pretrain. Use DataDeps to download automatically, if a model is not downlaod. the string should be in <code>pretrain&quot;&lt;model&gt;-&lt;model-name&gt;:&lt;item&gt;&quot;</code> format.</p><p>see also <code>Pretrain.pretrains()</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/9ce41cbab4028fa04a48680dbd46c1136b4b26cc/src/pretrain/Pretrain.jl#L32-L38">source</a></section><footer><hr/><a class="previous" href="../stacks/"><span class="direction">Previous</span><span class="title">Stacks</span></a><a class="next" href="../gpt/"><span class="direction">Next</span><span class="title">GPT</span></a></footer></article></body></html>
